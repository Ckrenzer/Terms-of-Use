---
title: "Terms of Service"
author: "Connor Krenzer"
date: "7/15/2021"
output: html_document
editor_options: 
  chunk_output_type: inline
---

# Introduction

Add writing here.

```{r setup-packages, include = FALSE}

knitr::opts_chunk$set(echo = FALSE,
                      include = FALSE,
                      results = "show",
                      error = FALSE,
                      warning = FALSE,
                      message = FALSE)


# PACKAGES
if(!require(pacman)) install.packages("pacman")

# Data Import Packages
pacman::p_load(readtext)

# General Packages
pacman::p_load(stringr, dplyr, ggplot2)

# Text Mining Packages
pacman::p_load(tidytext, textdata)

# Setting ggplot to the dark theme
ggplot2::theme_set(theme_dark())
# The colors that represent each company
company_colors <- c(Amazon = "#FF9900",
                    DuckDuckGo = "#ec2127",
                    Facebook = "#4267B2",
                    GitHub = "#000000",
                    LinkedIn = "#007bb6",
                    Spotify = "#1DB954",
                    YouTube = "#FF0000")




# SOURCING FILES
# This function comes courtesy of the folks writing the source() help page
sourceDir <- function(path, trace = TRUE, ...) {
  op <- options(); on.exit(options(op)) # to reset after each 
  for (nm in list.files(path, pattern = "[.][RrSsQq]$")) {
    if(trace) cat(nm,":")
    source(file.path(path, nm), ...)
    if(trace) cat("\n")
    options(op)
  }
}

# Loading in helper functions and related objects
sourceDir(path = "scripts/functions/")




# DATA IMPORT
tos <- readtext(file = "data/") %>% 
  rename(company = doc_id) %>% 
  mutate(company = str_remove_all(company, "_Terms_of_Service.txt"),
         text = str_to_lower(text))

```



```{r number of characters}

tos %>% 
  mutate(number_of_characters_in_TOS = str_length(text)) %>% 
  arrange(desc(number_of_characters_in_TOS)) %>% 
  ggplot() +
  geom_col(mapping = aes(x = reorder(company,
                                     number_of_characters_in_TOS),
                         y = number_of_characters_in_TOS,
                         fill = company),
           show.legend = FALSE) +
  ggtitle("Number of Characters Used in Terms of Service") +
  xlab(NULL) +
  ylab("Number of Characters") +
  scale_fill_manual(values = company_colors) +
  coord_flip()

```


Does this mean that refusing to track user data equates to needing fewer lawyers? I am inclined to think so, but the above graph is incapable of confirming this suspicion because of differences intrinsic in the services these companies provide. Further, Amazon--the largest company in the bunch--has a shorter terms of use page than its contemporaries. One explanation is that these companies spread a bunch of legal information off into different sections of its website to organize their policies into logical chunks.


# Data Prep

I will be using the Porter stemming algorithm to pre-process the text. While in an industrious mood, I decided to implement the algorithm myself using the [this article](https://vijinimallawaarachchi.com/2017/05/09/porter-stemming-algorithm/) and the [algorithm's official webpage](https://tartarus.org/martin/PorterStemmer/index.html) as guides. After many hours tweaking the intricacies of this algorithm, my implementation returns the same result as the official implementation in the [SnowballC package](https://CRAN.R-project.org/package=SnowballC) 98% of the time when using this dataset.

Doing this taught me the value of writing an algorithm yourself; rewriting and implementing an algorithm from scratch is a very effective way to learn how it works. Having the code to run the algorithm, **and** knowing the algorithm inside and out grants unique opportunities to make improvements that better suit your needs.



```{r unigrams and stemming}

unigrams <- tos %>% 
  unnest_tokens(word, text) %>% 
  porter_stemmer(unigram_df = unigrams)


ungrouped_unigram_counts <- count(unigrams, word, sort = FALSE)
unigram_counts <- count(unigrams, company, word, sort = FALSE)

```








```{r common words}

ungrouped_unigram_counts %>%  
  filter(!word %in% stop_words$word,
         word != "Ã¢") %>% 
  slice_max(order_by = n, n = 25) %>% 
  ggplot() +
  geom_col(mapping = aes(x = reorder(word, n), y = n, fill = word),
           show.legend = FALSE) +
  ggtitle("Most Common Words Among All TOS Policies") +
  xlab("Word") +
  ylab("Number of Uses") +
  coord_flip()




ungrouped_unigram_counts %>%  
  filter(!word %in% stop_words$word) %>% 
  slice_max(order_by = n, n = 25)

```


