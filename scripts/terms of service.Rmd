---
title: "Terms of Service"
author: "Connor Krenzer"
date: "7/15/2021"
output: html_document
editor_options: 
  chunk_output_type: inline
---

# Introduction

The next time you go on your favorite website, scroll down to the bottom of the page and look at all the links to different sections of the company. One of these will surely be the terms of service agreement that you consent to abide by while using their platform. No one bothers reading them unless he or she plans on doing something that has the potential to bring negative repercussions.

Much of the data I work with originates from web scraping. In the event you didn't know, most companies hate web scrapers for two reasons: (1) because of legitimate data privacy concerns, and (2) because companies want to sell the information that is easily acquired for free by a scraper. To put up barriers between data and people at least somewhat competent with a programming language, companies add red tape to their services in the form of a terms of use agreement. If you violate the terms of use, depending on the severity of the offense, you can get anything from suspended, banned, sued, or even criminally prosecuted. Make no mistake, there is a great deal of data out there that currently is and should continue to be off limits--public or not. It is the second case from which the bulk of my issues arise. Yes, intellectual property rights exist for good reason, but the problem for people like me, however, is that excessive red tape hinders those seeking to answer interesting questions.

The motivation for this investigation is a direct response to the anti-scraping policies so prevalent online. If the companies aren't going to allow people to scrape their content, I can at least scrape their terms of use! These documents will be used to draw conclusions about the companies more broadly.

### Links to Policies

This section is certain to become outdated in the future, so if you want to see the raw text used in this project, read the text files in the 'data' folder of [this project's GitHub repository](https://github.com/Ckrenzer/Terms-of-Use). The links used to get this information, however, are still provided below:

-   [LinkedIn's policy](https://www.linkedin.com/legal/user-agreement?trk=hb_ft_userag)

-   [Facebook's policy](https://www.facebook.com/legal/terms)

-   [YouTube's policy](https://www.youtube.com/t/terms)

-   [Amazon's policy](https://www.amazon.com/gp/help/customer/display.html?nodeId=508088&ref_=footer_cou)

-   [Spotify's policy](https://www.spotify.com/us/legal/end-user-agreement/)

-   [GitHub's policy](https://docs.github.com/en/github/site-policy/github-terms-of-service)

-   [DuckDuckGo's policy](https://duckduckgo.com/privacy)

I couldn't find a terms of use agreement for DuckDuckGo, likely because of the lax nature of their rules, so their privacy policy is used in place of a terms of use agreement. DuckDuckGo had to be included in this list as the 'outsider' that essentially lets you do whatever you want with the data collected from its search engine.


### A Message to the Reader

You may find this project reminiscent of the Fed Minutes Analysis. This study differs in several ways. Namely, several of the algorithms used here are written 'by-hand' without reliance on external libraries. Doing this has been incredibly helpful understanding the logic built into common frameworks for natural language processing and hopefully inspires the people reading this to try re-writing algorithms they use to gain a better understanding (or at least a greater appreciation!) of them. Further, the terms of use agreement is cross-sectional--we are not following the same organization across many years but are instead comparing different groups at one point in time. Finally--and most importantly, since we are ultimately concerned with results--Terms of Use aims to answer specific questions using text (as opposed to an 'aimless' exploration into the data).

```{r setup-packages, include = FALSE}

knitr::opts_chunk$set(echo = FALSE,
                      include = FALSE,
                      results = "show",
                      error = FALSE,
                      warning = FALSE,
                      message = FALSE)


# PACKAGES
if(!require(pacman)) install.packages("pacman")

# Data Import Packages
pacman::p_load(readtext)

# General Packages
pacman::p_load(stringr, dplyr, tidyr)

# Plotting Packages
pacman::p_load(ggplot2, patchwork)

# Text Mining Packages
pacman::p_load(tidytext, textdata)

# Setting ggplot to the dark theme
ggplot2::theme_set(theme_dark())
# The colors that represent each company
company_colors <- c(Amazon = "#FF9900",
                    DuckDuckGo = "#ec2127",
                    Facebook = "#4267B2",
                    GitHub = "#000000",
                    LinkedIn = "#007bb6",
                    Spotify = "#1DB954",
                    YouTube = "#FF0000")




# SOURCING FILES
# This function comes courtesy of the folks writing the source() help page
sourceDir <- function(path, trace = TRUE, ...) {
  op <- options(); on.exit(options(op)) # to reset after each 
  for (nm in list.files(path, pattern = "[.][RrSsQq]$")) {
    if(trace) cat(nm,":")
    source(file.path(path, nm), ...)
    if(trace) cat("\n")
    options(op)
  }
}

# Loading in helper functions and related objects
sourceDir(path = "scripts/functions/")




# DATA IMPORT
tos <- readtext(file = "data/") %>%
  as_tibble() %>% 
  rename(company = doc_id) %>% 
  mutate(company = str_remove_all(company, "_Terms_of_Service.txt"),
         text = str_to_lower(text),
         text = str_replace_all(text, "[â€œ§™“]", " "))

# company names after stemming
company_stems <- tos %>% 
  transmute(company = str_to_lower(company)) %>% 
  porter_stemmer(unigram_df = ., text_column = "company") %>% 
  pull(company)

```

# Document Lengths

To begin, why don't we calculate the length of each document? If you don't feel like navigating to the web pages hosting these documents yourself, this will save you some time! To calculate the length, the number of characters used in the document are counted. This includes all characters--newlines, punctuation, letters, digits, everything. While this may be considered imperfect, it gives us a 'good-enough' estimate for the length of each terms of use agreement.

Which company has the longest terms of use agreement?

```{r number of characters}

tos %>% 
  mutate(number_of_characters_in_TOS = str_length(text)) %>% 
  arrange(desc(number_of_characters_in_TOS)) %>% 
  ggplot() +
  geom_col(mapping = aes(x = reorder(company,
                                     number_of_characters_in_TOS),
                         y = number_of_characters_in_TOS,
                         fill = company),
           show.legend = FALSE) +
  ggtitle("Number of Characters Used in Terms of Service") +
  xlab(NULL) +
  ylab("Number of Characters") +
  scale_fill_manual(values = company_colors) +
  coord_flip()

```

Unsurprisingly, DuckDuckGo has the shortest policy. Does this mean that refusing to track user data equates to needing fewer lawyers? I am inclined to think so, but the above graph is incapable of confirming this suspicion because of differences intrinsic to the services provided by these companies. Further, Amazon--the largest company in the bunch by annual revenue--has a shorter terms of use page than its contemporaries. One explanation among many is that these companies spread legal documents out into different sections of their websites to organize policies into logical chunks.

# Data Prep

Before doing further work with these documents, they will need some pre-processing.

I will be using the Porter stemming algorithm to pre-process the text. While in an industrious mood, I decided to implement the algorithm myself using [this article](https://vijinimallawaarachchi.com/2017/05/09/porter-stemming-algorithm/) and the [algorithm's official web page](https://tartarus.org/martin/PorterStemmer/index.html) as guides. After many hours tweaking the algorithm's intricacies, my implementation returns the same result as the official implementation in the [SnowballC package](https://CRAN.R-project.org/package=SnowballC) 98% of the time when using this dataset.

Differences between the Snowball version and mine include:

-   Differences between the logic in the guides used (which walked through the original algorithm) and the slightly modified Snowball algorithm (the Snowball algorithm is also known as the 'Porter2' stemmer to differentiate it from the originally published algorithm).

-   Random errors that slip through the cracks due to regular expressions used, **m** calculation rules, and other oddities.

-   My version strips contractions before stemming ("aren't" becomes "are", "they'll" becomes "they", etc.).

-   My version uses a 'tidy' format--it takes a data frame and column name as input and return the original data frame after stemming the words in the specified column.

-   Speed. My version is about 300 times slower than the SnowballC package's implementation. This is primarily due to the copying of the data frame after each dplyr::mutate() call in my code. This difference is not very important here, since my version takes two seconds to run on this data set while Snowball takes .008 seconds. It is noticeably slower, but not enough for it to actually matter for this use-case.






Second, I will be using my own n-gram tokenizer instead of the tidytext package's unnest_tokens(). Several iterations of this algorithm were tried, with each one providing notable differences in performance. Since the theme here is a DIY approach to algorithms used, I am using the custom version that provides the most flexibility [(f5() from the benchmarking done with these algorithms)](https://github.com/Ckrenzer/Terms-of-Use/blob/main/scripts/performance%20evalutation/ngram%20tokenizers.R). Implementing this step was tricky but asking [a question on StackOverflow](https://stackoverflow.com/questions/68478351/regex-to-write-your-own-ngram-tokenizer) helped to sort things out. My approach to an n-gram tokenizer bases itself on the assembly of the n-grams from unigrams. Using a tokenizer allowing for greater flexibility dampened speed harshly; more restricted versions provide speed boosts in excess of an order of magnitude over unnest_tokens().






Doing this taught me the value of writing an algorithm yourself; rewriting and implementing an algorithm from scratch is a very effective way to learn how it works. Having the code to run the algorithm handy **and** knowing the algorithm inside and out grants unique opportunities to make improvements that better suit your needs.

# Word Counts

Onto the good stuff!

```{r unigrams}

unigrams <- tos %>% 
  mutate(text = str_replace_all(text, "[,/\\\\.:;()\"\\[\\]&]|^\\s*-\\s*|\u009d", " ")) %>% 
  ngrams(text_df = ., key_column = "company", text_column = "text", n = 1) %>% 
  filter(!text %in% stop_words$word) %>% 
  porter_stemmer(unigram_df = ., text_column = "text") %>% 
  filter(text != "",
         !text %in% company_stems,
         str_length(text) > 1)

ungrouped_unigram_counts <- count(unigrams, text, sort = FALSE)
unigram_counts <- count(unigrams, company, text, sort = FALSE)

```




```{r ungrouped common words}

ungrouped_unigram_counts %>% 
  slice_max(order_by = n, n = 25) %>% 
  ggplot() +
  geom_col(mapping = aes(x = reorder(text, n), y = n, fill = text),
           show.legend = FALSE) +
  ggtitle("Most Common Words Among All TOS Policies") +
  xlab("Word") +
  ylab("Number of Uses") +
  coord_flip()

```

```{r common words by company, fig.height = 9}

rank_words_by_company(unigram_counts, "Amazon") + rank_words_by_company(unigram_counts, "DuckDuckGo") + rank_words_by_company(unigram_counts, "Facebook") + rank_words_by_company(unigram_counts, "GitHub") + rank_words_by_company(unigram_counts, "LinkedIn") + rank_words_by_company(unigram_counts, "Spotify") + rank_words_by_company(unigram_counts, "YouTube") + plot_annotation(title = "Most Common Words in each User Agreement", caption = "Number of uses")

```



# TF-IDF

A great way to find differences between the documents is using the TF-IDF statistic.

```{r tfidf, fig.height = 9}

tf_idf_data <- tf_idf(text_df = unigram_counts, doc_id = "company", text_column = "text", counts_column = "n")


rank_words_by_company(word_count_df = tf_idf_data, company_name =  "Amazon", rank_column = "tf_idf", num_to_show = 5) + rank_words_by_company(word_count_df = tf_idf_data, company_name =  "DuckDuckGo", rank_column = "tf_idf", num_to_show = 5) + rank_words_by_company(word_count_df = tf_idf_data, company_name =  "Facebook", rank_column = "tf_idf", num_to_show = 5) + rank_words_by_company(word_count_df = tf_idf_data, company_name =  "GitHub", rank_column = "tf_idf", num_to_show = 5) + rank_words_by_company(word_count_df = tf_idf_data, company_name =  "LinkedIn", rank_column = "tf_idf", num_to_show = 5) + rank_words_by_company(word_count_df = tf_idf_data, company_name =  "Spotify", rank_column = "tf_idf", num_to_show = 5) + rank_words_by_company(word_count_df = tf_idf_data, company_name =  "YouTube", rank_column = "tf_idf", num_to_show = 5) + plot_annotation(title = "TF-IDF For Each User Agreement", caption = "TF-IDF Statistic")

```













# Skipgrams


```{r}

skipgrams <- tos %>% 
  mutate(text = str_replace_all(text, "[,/\\\\.:;()\"\\[\\]&]|^\\s+-\\s+|\u009d", " ")) %>% 
  ngrams(text_df = ., key_column = "company", text_column = "text", n = 8) %>% 
  mutate(skipgram_id = row_number()) %>% 
  unite(col = id, sep = "_", company, skipgram_id) %>% 
  ngrams(text_df = ., key_column = "id", text_column = "text", n = 1) %>% 
  pair_counts(df = ., doc_id = "company", text_column = "text") %>% 
  mutate(pairwise_prob = n / sum(n))

```


