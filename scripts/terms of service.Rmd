---
title: "Terms of Service"
author: "Connor Krenzer"
date: "7/15/2021"
output: html_document
editor_options: 
  chunk_output_type: inline
---

# Introduction

The next time you go on your favorite website, scroll down to the bottom of the page and look at all the links to different sections of the company. One of these will surely be the terms of service agreement that you consent to abide by while using their platform. No one bothers reading them unless he or she plans on doing something that has the potential to bring negative repercussions.

Much of the data I work with originates from web scraping. In the event you didn't know, most companies hate web scrapers for two reasons: (1) because of legitimate data privacy concerns, and (2) because companies want to sell the information that is easily acquired for free by a scraper. To put up barriers between data and people at least somewhat competent with a programming language, companies add red tape to their services in the form of a terms of use agreement. If you violate the terms of use, depending on the severity of the offense, you can get anything from suspended, banned, sued, or even criminally prosecuted. Make no mistake, there is a great deal of data out there that currently is and should continue to be off limits--public or not. It is the second case from which the bulk of my issues arise. Yes, intellectual property rights exist for good reason, but the problem for people like me, however, is that excessive red tape hinders us while seeking answers to interesting questions.

The motivation for this investigation is a direct response to the anti-scraping policies so prevalent online. If the companies aren't going to allow people to scrape their data, I can at least scrape their terms of use! These documents will be used to draw conclusions about the companies more broadly.

### Links to Policies

This section is certain to become outdated in the future, so if you want to see the raw text used in this project, read the text files in the 'data' folder of [this project's GitHub repository](https://github.com/Ckrenzer/Terms-of-Use). The links used to get this information, however, are still provided below:

-   [LinkedIn's policy](https://www.linkedin.com/legal/user-agreement?trk=hb_ft_userag)

-   [Facebook's policy](https://www.facebook.com/legal/terms)

-   [YouTube's policy](https://www.youtube.com/t/terms)

-   [Amazon's policy](https://www.amazon.com/gp/help/customer/display.html?nodeId=508088&ref_=footer_cou)

-   [Spotify's policy](https://www.spotify.com/us/legal/end-user-agreement/)

-   [GitHub's policy](https://docs.github.com/en/github/site-policy/github-terms-of-service)

-   [DuckDuckGo's policy](https://duckduckgo.com/privacy)

I couldn't find a terms of use agreement for DuckDuckGo, likely because of the lax nature of their rules, so their privacy policy is used in place of a terms of use agreement. DuckDuckGo had to be included in this list as the 'outsider' that essentially lets you do whatever you want with the data collected from its search engine.

```{r setup-packages, include = FALSE}

knitr::opts_chunk$set(echo = FALSE,
                      include = FALSE,
                      results = "show",
                      error = FALSE,
                      warning = FALSE,
                      message = FALSE)


# PACKAGES
if(!require(pacman)) install.packages("pacman")

# Data Import Packages
pacman::p_load(readtext)

# General Packages
pacman::p_load(stringr, dplyr)

# Plotting Packages
pacman::p_load(ggplot2, patchwork)

# Text Mining Packages
pacman::p_load(tidytext, textdata)

# Setting ggplot to the dark theme
ggplot2::theme_set(theme_dark())
# The colors that represent each company
company_colors <- c(Amazon = "#FF9900",
                    DuckDuckGo = "#ec2127",
                    Facebook = "#4267B2",
                    GitHub = "#000000",
                    LinkedIn = "#007bb6",
                    Spotify = "#1DB954",
                    YouTube = "#FF0000")




# SOURCING FILES
# This function comes courtesy of the folks writing the source() help page
sourceDir <- function(path, trace = TRUE, ...) {
  op <- options(); on.exit(options(op)) # to reset after each 
  for (nm in list.files(path, pattern = "[.][RrSsQq]$")) {
    if(trace) cat(nm,":")
    source(file.path(path, nm), ...)
    if(trace) cat("\n")
    options(op)
  }
}

# Loading in helper functions and related objects
sourceDir(path = "scripts/functions/")




# DATA IMPORT
tos <- readtext(file = "data/") %>% 
  rename(company = doc_id) %>% 
  mutate(company = str_remove_all(company, "_Terms_of_Service.txt"),
         text = str_to_lower(text))

```

# Document Lengths

To begin, why don't we calculate the length of each document? If you don't feel like navigating to the web pages hosting these documents yourself, this will save you some time! To calculate the length, the number of characters used in the document are counted. This includes all characters--newlines, punctuation, letters, digits, everything. While this may be considered imperfect, it gives us a 'good-enough' rough estimate for the length of each terms of use agreement.

Which company has the longest terms of use agreement?

```{r number of characters}

tos %>% 
  mutate(number_of_characters_in_TOS = str_length(text)) %>% 
  arrange(desc(number_of_characters_in_TOS)) %>% 
  ggplot() +
  geom_col(mapping = aes(x = reorder(company,
                                     number_of_characters_in_TOS),
                         y = number_of_characters_in_TOS,
                         fill = company),
           show.legend = FALSE) +
  ggtitle("Number of Characters Used in Terms of Service") +
  xlab(NULL) +
  ylab("Number of Characters") +
  scale_fill_manual(values = company_colors) +
  coord_flip()

```

Unsurprisingly, DuckDuckGo has the shortest policy. Does this mean that refusing to track user data equates to needing fewer lawyers? I am inclined to think so, but the above graph is incapable of confirming this suspicion because of differences intrinsic to the services provided by these companies. Further, Amazon--the largest company in the bunch by annual revenue--has a shorter terms of use page than its contemporaries. One explanation among many is that these companies spread legal documents out into different sections of their websites to organize policies into logical chunks.

# Data Prep

Before doing further work with these documents, they will need some pre-processing.

I will be using the Porter stemming algorithm to pre-process the text. While in an industrious mood, I decided to implement the algorithm myself using [this article](https://vijinimallawaarachchi.com/2017/05/09/porter-stemming-algorithm/) and the [algorithm's official web page](https://tartarus.org/martin/PorterStemmer/index.html) as guides. After many hours tweaking the algorithm's intricacies, my implementation returns the same result as the official implementation in the [SnowballC package](https://CRAN.R-project.org/package=SnowballC) 98% of the time when using this dataset.

Differences between the Snowball version and mine include:

-   Differences between the logic in the guides used (which walked through the original algorithm) and the slightly modified Snowball algorithm (the Snowball algorithm is also known as the 'Porter2' stemmer to differentiate it from the originally published algorithm).

-   Random errors that slip through the cracks due to regular expressions used, **m** calculation rules, and other oddities.

-   My version strips contractions before stemming ("aren't" becomes "are", "they'll" becomes "they", etc.).

-   My version uses a 'tidy' format--it takes a data frame and column name as input and return the original data frame after stemming the words in the specified column.

-   Speed. My version is about 300 times slower than the SnowballC package's implementation. This is primarily due to the copying of the data frame after each dplyr::mutate() call in my code. This difference is not very important here, since my version takes two seconds to run on this data set while Snowball takes .008 seconds. It is noticeably slower, but not enough for it to actually matter for this use-case.

Doing this taught me the value of writing an algorithm yourself; rewriting and implementing an algorithm from scratch is a very effective way to learn how it works. Having the code to run the algorithm handy **and** knowing the algorithm inside and out grants unique opportunities to make improvements that better suit your needs.

# Word Counts

Onto the good stuff!

```{r unigrams}

unigrams <- tos %>% 
  unnest_tokens(word, text) %>% 
  filter(!word %in% stop_words$word) %>% 
  porter_stemmer(unigram_df = ., text_column = "word")


ungrouped_unigram_counts <- count(unigrams, word, sort = FALSE)
unigram_counts <- count(unigrams, company, word, sort = FALSE)

```

```{r ungrouped common words}

ungrouped_unigram_counts %>%  
  filter(!word %in% stop_words$word,
         word != "â") %>% 
  slice_max(order_by = n, n = 25) %>% 
  ggplot() +
  geom_col(mapping = aes(x = reorder(word, n), y = n, fill = word),
           show.legend = FALSE) +
  ggtitle("Most Common Words Among All TOS Policies") +
  xlab("Word") +
  ylab("Number of Uses") +
  coord_flip()

```

```{r common words by company, fig.height = 9}

common_words_by_company <- function(df, company_name){
  
  df %>% 
    filter(!word %in% stop_words$word,
           word != "â",
           company == company_name) %>% 
    slice_max(order_by = n, n = 15) %>% 
    ggplot() +
    geom_col(mapping = aes(x = reorder(word, n), y = n, fill = company),
             show.legend = FALSE) +
    ggtitle(company_name) +
    xlab(NULL) +
    ylab("Number of Uses") +
    scale_fill_manual(values = company_colors[company_name]) +
    coord_flip() %>% 
    return()
  
}

common_words_by_company(unigram_counts, "Amazon") + common_words_by_company(unigram_counts, "DuckDuckGo") + common_words_by_company(unigram_counts, "Facebook") + common_words_by_company(unigram_counts, "GitHub") + common_words_by_company(unigram_counts, "LinkedIn") + common_words_by_company(unigram_counts, "Spotify") + common_words_by_company(unigram_counts, "YouTube") + plot_annotation(title = "Most Common Words in each User Agreement")

```
